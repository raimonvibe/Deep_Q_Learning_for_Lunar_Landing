{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQN4KF_Glp7U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "\"\"\"\n",
        "üöÄ LUNAR LANDER DQN TUTORIAL üåô\n",
        "===============================\n",
        "A comprehensive implementation of Deep Q-Learning for the Lunar Lander environment!\n",
        "\n",
        "üéÆ GAME OBJECTIVE:\n",
        "----------------\n",
        "Land the lunar module safely on the landing pad!\n",
        "- Control thrust and rotation\n",
        "- Manage fuel consumption\n",
        "- Avoid crashes and achieve smooth landings\n",
        "\n",
        "üß† LEARNING COMPONENTS:\n",
        "--------------------\n",
        "1. ü§ñ Deep Q-Network (DQN)\n",
        "   - Neural network for Q-value approximation\n",
        "   - Experience replay for stable learning\n",
        "   - Target network for reduced variance\n",
        "\n",
        "2. üéØ Training Process\n",
        "   - Epsilon-greedy exploration\n",
        "   - Bellman equation updates\n",
        "   - Gradient descent optimization\n",
        "\n",
        "3. üîÑ Key Algorithms\n",
        "   - Deep Q-Learning\n",
        "   - Experience Replay\n",
        "   - Soft Target Updates\n",
        "\"\"\"\n",
        "\n",
        "# üìö IMPORT REQUIRED LIBRARIES üìö\n",
        "import os                           # For handling file paths and directory operations\n",
        "import random                       # For generating random numbers in exploration\n",
        "import numpy as np                  # For efficient numerical operations\n",
        "import torch                        # The main PyTorch library for deep learning\n",
        "import torch.nn as nn              # Neural network modules from PyTorch\n",
        "import torch.optim as optim        # Optimization algorithms from PyTorch\n",
        "import gymnasium as gym            # The environment simulation library\n",
        "from collections import deque      # For implementing the replay buffer\n",
        "import matplotlib.pyplot as plt    # For plotting training results\n",
        "\n",
        "# üéÆ HYPERPARAMETERS üéÆ\n",
        "BUFFER_SIZE = int(1e5)    # Maximum size of experience replay buffer\n",
        "BATCH_SIZE = 64           # Number of experiences to sample from buffer\n",
        "GAMMA = 0.99              # Discount factor for future rewards\n",
        "TAU = 1e-3               # Soft update parameter for target network\n",
        "LR = 5e-4                # Learning rate for the optimizer\n",
        "UPDATE_EVERY = 4         # How often to update the network\n",
        "EPSILON_START = 1.0      # Starting value of epsilon (for exploration)\n",
        "EPSILON_END = 0.01       # Minimum value of epsilon\n",
        "EPSILON_DECAY = 0.995    # Decay rate of epsilon\n",
        "\n",
        "# üß† NEURAL NETWORK ARCHITECTURE üß†\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network (DQN) Architecture\n",
        "    ================================\n",
        "    Neural network that approximates the Q-function, mapping states to action values.\n",
        "\n",
        "    ARCHITECTURE DETAILS:\n",
        "    -------------------\n",
        "    1. Network Structure:\n",
        "       - Input Layer: State dimensions (8 for Lunar Lander)\n",
        "       - Hidden Layer 1: 64 neurons with ReLU activation\n",
        "       - Hidden Layer 2: 64 neurons with ReLU activation\n",
        "       - Output Layer: Action dimensions (4 for Lunar Lander)\n",
        "\n",
        "    2. Layer Choices:\n",
        "       - Fully connected layers for flexibility in learning\n",
        "       - ReLU activation for non-linearity\n",
        "       - No dropout (stability in Q-learning more important than regularization)\n",
        "\n",
        "    3. Output Interpretation:\n",
        "       - Each output neuron represents Q-value for one action\n",
        "       - Q-value predicts total future rewards for taking that action\n",
        "       - Highest Q-value indicates best predicted action\n",
        "\n",
        "    MATHEMATICAL BASIS:\n",
        "    -----------------\n",
        "    Q(s,a) ‚âà Neural_Network(s)[a]\n",
        "    - s: State input\n",
        "    - a: Action index\n",
        "    - Q(s,a): Expected future rewards for action a in state s\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"\n",
        "        üèóÔ∏è Initialize the Q-Network architecture\n",
        "\n",
        "        Parameters:\n",
        "        - state_size: Input dimensions (8 for Lunar Lander)\n",
        "        - action_size: Output dimensions (4 for Lunar Lander)\n",
        "\n",
        "        Architecture:\n",
        "        Input(state_size) -> FC(64) -> ReLU -> FC(64) -> ReLU -> FC(action_size)\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)     # First fully connected layer\n",
        "        self.fc2 = nn.Linear(64, 64)             # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(64, action_size)    # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        üîÑ Forward pass through the network\n",
        "\n",
        "        Process:\n",
        "        1. Input state tensor flows through layers\n",
        "        2. ReLU activation applied after each hidden layer\n",
        "        3. Final layer outputs Q-values for each action\n",
        "\n",
        "        Parameter:\n",
        "        - x: State tensor of shape (batch_size, state_size)\n",
        "\n",
        "        Returns:\n",
        "        - Q-values tensor of shape (batch_size, action_size)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))    # First layer + ReLU\n",
        "        x = torch.relu(self.fc2(x))    # Second layer + ReLU\n",
        "        return self.fc3(x)             # Output Q-values\n",
        "\n",
        "# üóÉÔ∏è REPLAY BUFFER IMPLEMENTATION üóÉÔ∏è\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer\n",
        "    =======================\n",
        "    A circular buffer that stores and samples experiences for training.\n",
        "\n",
        "    KEY CONCEPTS:\n",
        "    ------------\n",
        "    1. Experience Storage:\n",
        "       - Stores (state, action, reward, next_state, done) tuples\n",
        "       - Uses deque with maxlen for automatic FIFO behavior\n",
        "       - Efficiently manages memory by overwriting oldest experiences\n",
        "\n",
        "    2. Random Sampling:\n",
        "       - Breaks correlation between consecutive experiences\n",
        "       - Improves learning stability by providing diverse experiences\n",
        "       - Enables mini-batch learning for better generalization\n",
        "\n",
        "    3. Data Processing:\n",
        "       - Converts experiences to PyTorch tensors\n",
        "       - Handles batch processing efficiently\n",
        "       - Ensures proper data types for neural network input\n",
        "\n",
        "    IMPLEMENTATION DETAILS:\n",
        "    ---------------------\n",
        "    - Buffer Size: Maximum number of experiences to store\n",
        "    - Batch Size: Number of random experiences sampled for learning\n",
        "    - Experience Format: (state, action, reward, next_state, done)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        \"\"\"\n",
        "        üèóÔ∏è Initialize the replay buffer\n",
        "        Parameter:\n",
        "        - buffer_size: Maximum number of experiences to store (FIFO queue)\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=buffer_size)    # Create deque with max length\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        ‚ûï Add a new experience to memory\n",
        "\n",
        "        Experience tuple format:\n",
        "        - state: Current state observation (numpy array)\n",
        "        - action: Action taken (integer)\n",
        "        - reward: Reward received (float)\n",
        "        - next_state: Next state observation (numpy array)\n",
        "        - done: Episode termination flag (boolean)\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        üé≤ Random sampling of experiences from memory\n",
        "\n",
        "        Process:\n",
        "        1. Randomly select 'batch_size' experiences\n",
        "        2. Unzip experiences into separate arrays\n",
        "        3. Convert arrays to PyTorch tensors\n",
        "        4. Return batch of experiences for learning\n",
        "\n",
        "        Parameter:\n",
        "        - batch_size: Number of experiences to sample\n",
        "\n",
        "        Returns:\n",
        "        - Tuple of (states, actions, rewards, next_states, dones) as tensors\n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=batch_size)\n",
        "\n",
        "        # Convert experiences to torch tensors for batch processing\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float()\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        üìè Return current size of memory\n",
        "        Returns:\n",
        "        - Current number of experiences stored\n",
        "        \"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "# ü§ñ DQN AGENT IMPLEMENTATION ü§ñ\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Learning Agent\n",
        "    ====================\n",
        "    This agent learns to control the lunar lander using Deep Q-Learning algorithm.\n",
        "    It maintains two networks (local and target) for stable training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"\n",
        "        üèóÔ∏è Initialize the DQN Agent\n",
        "        Parameters:\n",
        "        - state_size: Dimension of each state\n",
        "        - action_size: Dimension of each action\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # üß† Create the Neural Networks (local and target)\n",
        "        self.local_net = DQN(state_size, action_size)    # Network for selecting actions\n",
        "        self.target_net = DQN(state_size, action_size)   # Target network for stable Q-values\n",
        "        self.optimizer = optim.Adam(self.local_net.parameters(), lr=LR)  # Adam optimizer\n",
        "\n",
        "        # üóÉÔ∏è Create Replay Buffer\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "        # üìä Initialize time step for updating target network\n",
        "        self.t_step = 0\n",
        "\n",
        "        # üé≤ Initialize epsilon for exploration\n",
        "        self.epsilon = EPSILON_START\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        üë£ Take a step in the environment and handle learning process\n",
        "\n",
        "        PROCESS FLOW:\n",
        "        -------------\n",
        "        1. Experience Storage:\n",
        "           - Save (state, action, reward, next_state, done) tuple in replay buffer\n",
        "           - This experience can be used later for batch learning\n",
        "\n",
        "        2. Learning Timing:\n",
        "           - Learn every UPDATE_EVERY time steps\n",
        "           - This reduces computation and allows for batch processing\n",
        "           - Requires minimum buffer size to ensure enough random samples\n",
        "\n",
        "        Parameters:\n",
        "        - state: Current state observation\n",
        "        - action: Action taken in current state\n",
        "        - reward: Reward received from environment\n",
        "        - next_state: Next state observation\n",
        "        - done: Boolean indicating if episode ended\n",
        "        \"\"\"\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0 and len(self.memory) > BATCH_SIZE:\n",
        "            experiences = self.memory.sample(BATCH_SIZE)\n",
        "            self.learn(experiences)\n",
        "\n",
        "    def act(self, state, eval_mode=False):\n",
        "        \"\"\"\n",
        "        üéØ Select an action using epsilon-greedy policy\n",
        "\n",
        "        EPSILON-GREEDY STRATEGY:\n",
        "        ----------------------\n",
        "        1. Exploration (Random Action):\n",
        "           - Probability = epsilon\n",
        "           - Allows discovery of new strategies\n",
        "           - Epsilon decays over time (from EPSILON_START to EPSILON_END)\n",
        "\n",
        "        2. Exploitation (Best Known Action):\n",
        "           - Probability = 1 - epsilon\n",
        "           - Uses neural network to predict best action\n",
        "           - Network outputs Q-values for each possible action\n",
        "\n",
        "        Parameters:\n",
        "        - state: Current state observation\n",
        "        - eval_mode: If True, use greedy policy (no exploration)\n",
        "\n",
        "        Returns:\n",
        "        - action: Selected action index\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)  # Convert state to tensor\n",
        "\n",
        "        # Set network to evaluation mode and get action values\n",
        "        self.local_net.eval()  # Disable dropout/batchnorm for prediction\n",
        "        with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "            action_values = self.local_net(state)\n",
        "        self.local_net.train()  # Re-enable training mode\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if not eval_mode and random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_size)    # Random action (exploration)\n",
        "        return np.argmax(action_values.cpu().data.numpy())    # Best action (exploitation)\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        \"\"\"\n",
        "        üß† LEARNING PROCESS - WHERE THE MAGIC HAPPENS! ‚ú®\n",
        "\n",
        "        SUPER AWESOME ALGORITHM STEPS:\n",
        "        ---------------------------\n",
        "        1. üì¶ Unpack Experiences:\n",
        "           - Get (state, action, reward, next_state, done) from batch\n",
        "           - Convert everything to proper tensor format\n",
        "\n",
        "        2. üéØ Get Q-Values:\n",
        "           - Current Q-values from local network\n",
        "           - Next Q-values from target network (no_grad for stability)\n",
        "\n",
        "        3. üîÆ Calculate Target Q-Values:\n",
        "           - Use Bellman equation: Q = R + Œ≥ * max(Q_next)\n",
        "           - Handle terminal states (done = True)\n",
        "\n",
        "        4. üìà Update Network:\n",
        "           - Calculate loss between current and target Q-values\n",
        "           - Perform gradient descent step\n",
        "           - Update target network softly\n",
        "\n",
        "        5. üé≠ Epsilon Update:\n",
        "           - Decay exploration rate over time\n",
        "           - Balance exploration vs exploitation\n",
        "\n",
        "        MATHEMATICAL MAGIC ‚ú®:\n",
        "        -------------------\n",
        "        Q_target = R + Œ≥ * max(Q_next) * (1 - done)\n",
        "        Loss = MSE(Q_current, Q_target)\n",
        "\n",
        "        Parameters:\n",
        "        - experiences: Tuple of (states, actions, rewards, next_states, dones)\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # üéØ Get max predicted Q-values (for next states) from target model\n",
        "        Q_targets_next = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # üîÆ Compute Q targets for current states\n",
        "        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # üé≠ Get expected Q values from local model\n",
        "        Q_expected = self.local_net(states).gather(1, actions)\n",
        "\n",
        "        # üìà Compute loss and perform optimization step\n",
        "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # üîÑ Update target network\n",
        "        self.soft_update()\n",
        "\n",
        "        # üé≠ Update epsilon\n",
        "        self.epsilon = max(EPSILON_END, EPSILON_DECAY * self.epsilon)\n",
        "\n",
        "    def soft_update(self):\n",
        "        \"\"\"\n",
        "        üîÑ Soft update of target network parameters\n",
        "        Œ∏_target = œÑ*Œ∏_local + (1 - œÑ)*Œ∏_target\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(self.target_net.parameters(),\n",
        "                                           self.local_net.parameters()):\n",
        "            target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "# üéÆ TRAINING FUNCTION üéÆ\n",
        "def train_agent(n_episodes=2000):\n",
        "    \"\"\"\n",
        "    üèÉ‚Äç‚ôÇÔ∏è Train the DQN agent\n",
        "    Parameter:\n",
        "    - n_episodes: Number of episodes to train\n",
        "    Returns:\n",
        "    - scores: List of scores from each episode\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    env = gym.make('LunarLander-v3')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Create agent\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    # Initialize score list\n",
        "    scores = []\n",
        "\n",
        "    # Training loop\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        # Episode loop\n",
        "        while not done:\n",
        "            # Select and perform action\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update agent\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # Update state and score\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        # Store score\n",
        "        scores.append(score)\n",
        "\n",
        "        # Print progress\n",
        "        if i_episode % 100 == 0:\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            print(f'Episode {i_episode}\\tAverage Score: {avg_score:.2f}')\n",
        "\n",
        "    # Save trained model\n",
        "    torch.save(agent.local_net.state_dict(), 'lunar_lander_dqn.pth')\n",
        "\n",
        "    return scores\n",
        "\n",
        "# üìä VISUALIZATION TIME - LET'S SEE THOSE AWESOME RESULTS! üìà\n",
        "def plot_scores(scores):\n",
        "    \"\"\"\n",
        "    üìä VISUALIZATION TIME - LET'S SEE THOSE AWESOME RESULTS! üìà\n",
        "\n",
        "    PLOT DETAILS:\n",
        "    ------------\n",
        "    1. üìâ Score History:\n",
        "       - Raw scores for each episode\n",
        "       - Moving average for trend visualization\n",
        "       - Clear display for real-time updates\n",
        "\n",
        "    2. üé® Plot Features:\n",
        "       - Episode numbers on x-axis\n",
        "       - Scores on y-axis\n",
        "       - Running average line for smoothness\n",
        "\n",
        "    Parameter:\n",
        "    - scores: List of scores from all episodes\n",
        "    \"\"\"\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    plt.title('üöÄ Training Progress üìà')\n",
        "    plt.xlabel('Episode Number üéÆ')\n",
        "    plt.ylabel('Score üéØ')\n",
        "    plt.plot(scores)\n",
        "    plt.plot([np.mean(scores[max(0, i-100):i]) for i in range(1, len(scores)+1)])\n",
        "    plt.pause(0.001)\n",
        "\n",
        "# üé• SHOWTIME - RECORD OUR AWESOME AGENT IN ACTION! üé¨\n",
        "def record_trained_agent(agent, num_episodes=3):\n",
        "    \"\"\"\n",
        "    üé• SHOWTIME - RECORD OUR AWESOME AGENT IN ACTION! üé¨\n",
        "\n",
        "    RECORDING PROCESS:\n",
        "    ----------------\n",
        "    1. üéÆ Episode Setup:\n",
        "       - Create video recorder\n",
        "       - Reset environment\n",
        "       - Initialize state\n",
        "\n",
        "    2. üéØ Action Selection:\n",
        "       - Use trained agent\n",
        "       - No exploration (eval mode)\n",
        "       - Record each step\n",
        "\n",
        "    3. üìΩÔ∏è Video Creation:\n",
        "       - Save episode recordings\n",
        "       - Track performance\n",
        "       - Generate MP4 files\n",
        "\n",
        "    Parameters:\n",
        "    - agent: Trained DQN agent\n",
        "    - num_episodes: Number of episodes to record (default: 3)\n",
        "\n",
        "    Returns:\n",
        "    - scores: List of scores from recorded episodes\n",
        "    \"\"\"\n",
        "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "    scores = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.act(state, eval_mode=True)  # No exploration in evaluation\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            score += reward\n",
        "            state = next_state\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "        print(f'üé¨ Episode {i+1} Score: {score:.2f}')\n",
        "\n",
        "    env.close()\n",
        "    return scores\n",
        "\n",
        "# üöÄ MAIN EXECUTION üöÄ\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    üéÆ LET'S TRAIN OUR LUNAR LANDER! üåô\n",
        "\n",
        "    EXECUTION STEPS:\n",
        "    --------------\n",
        "    1. üîß Setup Environment\n",
        "    2. üéì Train Agent\n",
        "    3. üìä Plot Results\n",
        "    4. üé• Record Videos\n",
        "    \"\"\"\n",
        "    # üåü Create and train the agent\n",
        "    print(\"üöÄ Starting training... Hold onto your spacesuits! üßë‚ÄçüöÄ\")\n",
        "    scores, agent = train_agent()\n",
        "\n",
        "    # üìä Plot final results\n",
        "    print(\"\\nüìà Plotting training results...\")\n",
        "    plot_scores(scores)\n",
        "\n",
        "    # üé• Record some epic landings\n",
        "    print(\"\\nüé¨ Recording agent performances...\")\n",
        "    record_trained_agent(agent)\n",
        "\n",
        "    print(\"\\nüéâ Training complete! Check out those smooth landings! üåô\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y015CvhlsFm",
        "outputId": "4bf88b93-d7f2-4880-a918-3230416da129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting ale-py>=0.9 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,427 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349150 sha256=7123e10395f55d9d125c4748efd185093b97cfd6e56647c1b16f44cb1fea7804\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1.post0\n",
            "üöÄ Starting training... Hold onto your spacesuits! üßë‚ÄçüöÄ\n",
            "Episode 100\tAverage Score: -333.25\n",
            "Episode 200\tAverage Score: -162.94\n",
            "Episode 300\tAverage Score: -119.92\n",
            "Episode 400\tAverage Score: -113.05\n",
            "Episode 500\tAverage Score: -84.38\n",
            "Episode 600\tAverage Score: -66.26\n",
            "Episode 700\tAverage Score: -54.65\n",
            "Episode 800\tAverage Score: -44.74\n",
            "Episode 900\tAverage Score: -36.32\n"
          ]
        }
      ]
    }
  ]
}